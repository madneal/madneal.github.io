<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Filebeat on Neal&#39;s Blog</title>
    <link>https://madneal.com/tags/filebeat/</link>
    <description>Recent content in Filebeat on Neal&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>© This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License，please give source if you wish to quote or reproduce.</copyright>
    <lastBuildDate>Thu, 16 Nov 2017 09:11:39 +0000</lastBuildDate>
    <atom:link href="https://madneal.com/tags/filebeat/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>基于ELK进行邮箱访问日志的分析</title>
      <link>https://madneal.com/post/%E5%9F%BA%E4%BA%8Eelk%E8%BF%9B%E8%A1%8C%E9%82%AE%E7%AE%B1%E8%AE%BF%E9%97%AE%E6%97%A5%E5%BF%97%E7%9A%84%E5%88%86%E6%9E%90/</link>
      <pubDate>Thu, 16 Nov 2017 09:11:39 +0000</pubDate>
      <guid>https://madneal.com/post/%E5%9F%BA%E4%BA%8Eelk%E8%BF%9B%E8%A1%8C%E9%82%AE%E7%AE%B1%E8%AE%BF%E9%97%AE%E6%97%A5%E5%BF%97%E7%9A%84%E5%88%86%E6%9E%90/</guid>
      <description>&lt;p&gt;公司希望能够搭建自己的日志分析系统。现在基于ELK的技术分析日志的公司越来越多，在此也记录一下我利用ELK搭建的日志分析系统。&lt;/p&gt;&#xA;&lt;h2 id=&#34;系统搭建&#34;&gt;系统搭建&lt;/h2&gt;&#xA;&lt;p&gt;系统主要是基于elasticsearch+logstash+filebeat+kibana+nginx，其实我这个用的还是比较多的，可以直接用logstash直接去采集日志。不过由于logstash的性能影响都比较大，而且filebeat安装很方便，而且占用资源很小，所以现在filebeat现在被广泛应用于日志采集。&lt;/p&gt;&#xA;&lt;p&gt;其实在搭这个系统还是比较麻烦的，可是前面有的踩过的坑当时没有及时记录下来，有点忘记了。但是里面就是配置logstash和filebeat配置证书的时候有点麻烦，配置不好会一直没有办法连通。还要注意ES的索引占得空间，其实ES索引还蛮占空间的。&lt;/p&gt;&#xA;&lt;h2 id=&#34;logstash&#34;&gt;Logstash&lt;/h2&gt;&#xA;&lt;p&gt;Logstash其实在整个ELK中环节还蛮重要的，其实可以理解为一个“中间人”的角色。它通过从filebeat中接受数据，然后进行过滤，最后再传输给es。所以一般logstash的配置也包括input,output以及filter的配置。&lt;/p&gt;&#xA;&lt;h3 id=&#34;filter&#34;&gt;filter&lt;/h3&gt;&#xA;&lt;p&gt;logstash中的filter比较重要，可以对日志利用正则进行过滤，这样你可以更关心日志中你需要关注的字段。强烈建议去&lt;a href=&#34;http://grokdebug.herokuapp.com/patterns&#34;&gt;grokdebugger&lt;/a&gt;去调试你的grok正则表达式，但是国内访问速度比较慢，可以采取一定手段访问。上面还有grok内置的一些常用正则表达式，可以配合试用调试。&lt;/p&gt;&#xA;&lt;h3 id=&#34;geoip&#34;&gt;geoip&lt;/h3&gt;&#xA;&lt;p&gt;日志分析中往往涉及到ip归属地的查询。logstash自带的geoip插件已经自带了数据库，可以下载最新的&lt;a href=&#34;https://dev.maxmind.com/geoip/legacy/geolite/&#34;&gt;数据库&lt;/a&gt;。同时，geoip里面包含了很多信息，你可以进行过滤，只选择自己想要的字段：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;geoip {&#xA;    fields =&amp;gt; [&amp;#34;city_name&amp;#34;, &amp;#34;country_name&amp;#34;]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;日志分析&#34;&gt;日志分析&lt;/h2&gt;&#xA;&lt;p&gt;邮箱日志的格式是IIS的日至格式，日志是由空格分割开的一些字段信息。主要的字段包含以下这些字段信息：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#Fields: date time s-ip cs-method cs-uri-stem cs-uri-query s-port cs-username c-ip cs(User-Agent) sc-status sc-substatus sc-win32-status time-taken&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;针对这个日志，我利用grok去解析这些字段的信息，自定义的正则规则是：&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;DATE_CH   \d+[/-]\d+[/-]\d+&#xA;&#xA;OUTER_EMAIL %{DATE_CH:date} %{TIME:time} %{IP:serverIp} %{WORD:method} %{URIPATH:uristem} %{PARAM:query} %{INT:port} %{NOTSPACE:username} %{IP:clientIp} %{NOTSPACE:ua} %{INT:status} %{INT:substatus} %{INT:win32status} %{INT:timetaken}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过grok我们可以获取这些字段，但如何在这些字段中挖掘有用的信息呢？这里面比较有价值的信息就是用户的登录时间，登录客户端，以及登录的ip。通过之前的 geoip 的配置，我们可以获取到ip对应的地址信息。登录时间由于很多邮件客户端在后台会去同步或者去登陆，所以参考意义不是特别的大。&lt;/p&gt;&#xA;&lt;p&gt;后续对于日志如何进行分析，我目前还没有特别好的思路，希望有着方面经验的小伙伴可以一起交流。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
