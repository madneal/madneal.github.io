<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Neal&#39;s Blog</title>
    <link>https://madneal.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Neal&#39;s Blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>© This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License，please give source if you wish to quote or reproduce.</copyright>
    <lastBuildDate>Sun, 19 Apr 2015 12:52:51 +0000</lastBuildDate>
    <atom:link href="https://madneal.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>独立成分分析（Independent Component Analysis）</title>
      <link>https://madneal.com/post/%E7%8B%AC%E7%AB%8B%E6%88%90%E5%88%86%E5%88%86%E6%9E%90independent-component-analysis/</link>
      <pubDate>Sun, 19 Apr 2015 12:52:51 +0000</pubDate>
      <guid>https://madneal.com/post/%E7%8B%AC%E7%AB%8B%E6%88%90%E5%88%86%E5%88%86%E6%9E%90independent-component-analysis/</guid>
      <description>&lt;p&gt;ICA是一种用于在统计数据中寻找隐藏的因素或者成分的方法。ICA是一种广泛用于盲缘分离的(BBS)方法，用于揭示随机变量或者信号中隐藏的信息。ICA被用于从混合信号中提取独立的信号信息。ICA在20世纪80年代提出来，但是知道90年代中后期才开始逐渐流行起来。&#xA;ICA的起源可以来源于一个鸡尾酒会问题，我们假设三个观测点x1,x2,x3,放在房间里同时检测三个人说话，另三个人的原始信号为s1,s2,s3，则求解的过程可以如下图所示：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;http://img.blog.csdn.net/20150419121502150&#34; alt=&#34;鸡尾酒会问题&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;定义&#34;&gt;定义&lt;/h2&gt;&#xA;&lt;p&gt;假设n个随机变量x1,x2,&amp;hellip;.xn,由n个随机变量s1,s2,&amp;hellip;sn组成，并且这n个随机变量是相互独立的，可以用下面的公示表达：&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20150419121857576&#34; alt=&#34;定义公式&#34;&gt;&#xA;为了表达的方便，我们可以用向量的形式来表达：&#xA;&lt;strong&gt;x = As&lt;/strong&gt;&#xA;这个只不过是ICA最基本的定义，在很多实际问题中，应该包含了噪声。但是为了简化问题，我们这里忽略了噪声。因为如果模型中包含噪音，处理起来将会十分困难，而且大多数不包含噪音的模型已经能够解决很多问题，所以这里我们就将噪声先忽略。&lt;/p&gt;&#xA;&lt;h2 id=&#34;ica的限制条件&#34;&gt;ICA的限制条件&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;独立成分应该是相互之间独立的。这是ICA成立的基本原则，同时，基本上可以说只需要这个原则我们就可以估计这个模型。&lt;/li&gt;&#xA;&lt;li&gt;独立成分必须是非高斯分布的。高斯分布的高阶累计量是0，但是高阶信息对于ICA的模型的估计却是十分必要的。&lt;/li&gt;&#xA;&lt;li&gt;为了简化，我们假设未知的混合矩阵A是一个方阵。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;白化&#34;&gt;白化&lt;/h2&gt;&#xA;&lt;p&gt;白化是一种比不相关性要稍微强一些的性质。对一个零均值的随即向量y进行白化处理，就是让它的组成成分不相关，并且让变量的方差相等。也就是说，变量y的协方差矩阵是单位矩阵：&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20150419123019017&#34; alt=&#34;这里写图片描述&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;为什么独立成分是非高斯的&#34;&gt;为什么独立成分是非高斯的&lt;/h2&gt;&#xA;&lt;p&gt;ICA最基本的限制条件就是独立成分必须是非高斯分布的，这或许也是ICA早期没有流行起来的原因。我们假设变量x1和x2是高斯分布的，不相关的，且方差相等：&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20150419123222812&#34; alt=&#34;高斯分布&#34;&gt;&#xA;下面的图表示联合概率分布，可以看出，我们无法判断任何关于变量x1和x2的方向信息，这就是为什么混合矩阵A不能被估计出来的原因：&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20150419123533326&#34; alt=&#34;这里写图片描述&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;峭度&#34;&gt;峭度&lt;/h2&gt;&#xA;&lt;p&gt;在这我们讲述一个利用峭度来进行ICA模型估计的方法，ICA的估计方法很多，这只是最基础的一个方法。&#xA;对于变量y峭度可以由下面的公式定义：&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20150419123638933&#34; alt=&#34;这里写图片描述&#34;&gt;&#xA;峭度是可正可负的，高斯分布变量的峭度是0，这也是为什么独立成分必须是非高斯分布的原因之一。峭度为负的变量分布称为次高斯分布，峭度为正的变量的分布则为超高斯分布，下图分别是拉普拉斯分布（超高斯分布）和均匀分布（次高斯分布）：&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20150419124105840&#34; alt=&#34;这里写图片描述&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;基于峭度的梯度算法&#34;&gt;基于峭度的梯度算法&lt;/h2&gt;&#xA;&lt;p&gt;我们经常利用峭度的绝对值或者平方来进行求解：&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20150419124252155&#34; alt=&#34;这里写图片描述&#34;&gt;&#xA;我们通过优化这个目标函数来估计ICA模型，z表示白化后的观察数据x。&#xA;实际上，我们是使峭度极大化。我们会从某个方向向量w开始，然后计算在什么方向峭度的增长最快，我们则将方向向量w向这个方向移动。&#xA;峭度绝对值的梯度可以如下计算：&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20150419124604628&#34; alt=&#34;这里写图片描述&#34;&gt;&#xA;下面是一个快速不动点算法基于峭度计算的流程图：&#xA;&lt;img src=&#34;http://img.blog.csdn.net/20150419124734063&#34; alt=&#34;这里写图片描述&#34;&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;## ICA估计的主要方法 ##&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;通过极大化非高斯性来估计&lt;/li&gt;&#xA;&lt;li&gt;通过极大似然性来估计&lt;/li&gt;&#xA;&lt;li&gt;通过极小互信息来估计&lt;/li&gt;&#xA;&lt;li&gt;通过张量的方法来估计&lt;/li&gt;&#xA;&lt;li&gt;通过非线性分解和非线性PCA来估计&#xA;这里，我们只是讲了其中的一个基础方法之一，并不就是最好的方法。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;ICA算法的思想可以用下面的公式来描述：&#xA;ICA method = objective funtion + optimization algorithm&lt;/p&gt;&#xA;&lt;h2 id=&#34;引用&#34;&gt;引用&lt;/h2&gt;&#xA;&lt;p&gt;[1]Hyvärinen A, Karhunen J, Oja E. Independent component analysis[M]. John Wiley &amp;amp; Sons, 2004.&#xA;[2]Hyvärinen A, Oja E. Independent component analysis: algorithms and applications[J]. Neural networks, 2000, 13(4): 411-430.&#xA;[3]Hyvärinen A, Oja E. A fast fixed-point algorithm for independent component analysis[J]. Neural computation, 1997, 9(7): 1483-1492.&#xA;[4]王刚. 基于最大非高斯估计的独立分量分析理论研究 [D][D]. 国防科学技术大学, 2005.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
