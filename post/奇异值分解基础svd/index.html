<!DOCTYPE html>
<html lang="en">
<head>

  <meta charset="utf-8" />

  
  <title>奇异值分解基础(SVD)</title>

  
  





  
  <meta name="author" content="Neal" />
  <meta name="description" content="最近要了解一下Incremental PCA的一些知识，然后看到一篇论文里面讲到了SVD（奇异值分解），奈何自己以前没有把机器学习的课好好上，现在很多东西还是要补回来。所以，我就想了解一些SVD的基础知识。 PCA的实现一般有两种方法，一种是用特征值分解去实现，一种是用奇异值分解去实现的，SVD貌似在很多领域都有很重要的应用。
特征值和特征向量 特征值和特征向量是线性代数里面的基础知识，相信大部分人都知道： 很显然，λ就是特征向量v对应的特征值，一个矩阵的一组特征向量都是相互正交的，相信这些大家在线性代数都有学习。特征值分解是将一个矩阵以下面的形式进行分解： 其中Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角矩阵，每一个对角线上的元素就是一个特征值。 特征值分解可以得到特征值和特征向量，特征值表示的是这个特征值的重要性，而特征向量表示的是这个特征是什么，可以将每一个特征向量理解为一个线性的子空间。不过特征值分解也有很多的局限，比如变换的矩阵必须是方阵。
奇异值 特征值分解只能针对于方阵，局限性较大，而奇异值分解是一个能够用于任意的矩阵的一种分解方法： 假设A是一个N*M的矩阵，那么U是一个N*N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N*M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V&amp;rsquo;（V的转置矩阵）是一个N*N的矩阵，里面的向量也是正教的，称为右奇异向量。 我们将矩阵A和他的转置矩阵相乘，就可以得到一个方阵，我们利用方阵的求特征值可以得到： 这里面的v，就是我们上面所说的右奇异向量，由此我们可以得到 这里的σ就是上面所说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从打到小排列，而且σ的减少特别的快。在很多情况下，前10%的甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们可以用前r大的奇异值来近似描述矩阵，因此部分奇异值分解可以如下定义： r是一个远小于m、n的数， 右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，r越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和要远远小于原始的矩阵A。
SVD和PCA PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于机器学习的数据，方差大反而有意义，不然输入的数据就是同一个点了，那方差九尾0了。 这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假设我们想用一条直线去拟合这些点，那我们应该选择什么方向的线？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴和y轴上得到的方差就是相似的。 一般来说方差大的方向就是信号的方向，方差小的方向就是噪声的方向，我们在数据挖掘或者数字信号处理中，往往是要提高信噪比。就上图说，如果我们只保留signal方向的数据，就可以对原始数据进行不错的近似了。 PCA的就是对原始的空间中顺序地找一组相互正教的坐标轴，第一个轴使得方差最大，第二个轴是在与第一个轴相交的平面中使得方差最大，第三个轴也是在与第1,2个轴正交的平面中使得方差最大，这种假设在N维空间中，我们就可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间，但是我们可以选择r个坐标轴使得空间的压缩使得数据的损失最小。 假设我们矩阵的每一行代表一个样本，每一列代表一个feature，将一个m*n的矩阵A进行坐标轴的变化，P就是一个变换的矩阵从一个n维的空间变换到另外一个n维的空间 而将一个m*n的矩阵A变成一个m*r的矩阵，我们就会使得本来有n个feature的，变成有r个feature了(r小于n)，这r个其实就是对n个feature的一种提炼，我们把这个称为feature的压缩： 之前的SVD的式子是： 在矩阵的两边同时乘上一个矩阵V，由于v是一个正交的矩阵 我们对SVD分解的式子两边乘以U的转置矩阵U&amp;rsquo; PCA几乎可以说是对SVD的一种包装，如果我们实现了SVD，那也就实现了PCA。" />

  
  
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@gohugoio" />
    <meta name="twitter:title" content="奇异值分解基础(SVD)" />
    <meta name="twitter:description" content="最近要了解一下Incremental PCA的一些知识，然后看到一篇论文里面讲到了SVD（奇异值分解），奈何自己以前没有把机器学习的课好好上，现在很多东西还是要补回来。所以，我就想了解一些SVD的基础知识。 PCA的实现一般有两种方法，一种是用特征值分解去实现，一种是用奇异值分解去实现的，SVD貌似在很多领域都有很重要的应用。
特征值和特征向量 特征值和特征向量是线性代数里面的基础知识，相信大部分人都知道： 很显然，λ就是特征向量v对应的特征值，一个矩阵的一组特征向量都是相互正交的，相信这些大家在线性代数都有学习。特征值分解是将一个矩阵以下面的形式进行分解： 其中Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角矩阵，每一个对角线上的元素就是一个特征值。 特征值分解可以得到特征值和特征向量，特征值表示的是这个特征值的重要性，而特征向量表示的是这个特征是什么，可以将每一个特征向量理解为一个线性的子空间。不过特征值分解也有很多的局限，比如变换的矩阵必须是方阵。
奇异值 特征值分解只能针对于方阵，局限性较大，而奇异值分解是一个能够用于任意的矩阵的一种分解方法： 假设A是一个N*M的矩阵，那么U是一个N*N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N*M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V&amp;rsquo;（V的转置矩阵）是一个N*N的矩阵，里面的向量也是正教的，称为右奇异向量。 我们将矩阵A和他的转置矩阵相乘，就可以得到一个方阵，我们利用方阵的求特征值可以得到： 这里面的v，就是我们上面所说的右奇异向量，由此我们可以得到 这里的σ就是上面所说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从打到小排列，而且σ的减少特别的快。在很多情况下，前10%的甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们可以用前r大的奇异值来近似描述矩阵，因此部分奇异值分解可以如下定义： r是一个远小于m、n的数， 右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，r越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和要远远小于原始的矩阵A。
SVD和PCA PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于机器学习的数据，方差大反而有意义，不然输入的数据就是同一个点了，那方差九尾0了。 这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假设我们想用一条直线去拟合这些点，那我们应该选择什么方向的线？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴和y轴上得到的方差就是相似的。 一般来说方差大的方向就是信号的方向，方差小的方向就是噪声的方向，我们在数据挖掘或者数字信号处理中，往往是要提高信噪比。就上图说，如果我们只保留signal方向的数据，就可以对原始数据进行不错的近似了。 PCA的就是对原始的空间中顺序地找一组相互正教的坐标轴，第一个轴使得方差最大，第二个轴是在与第一个轴相交的平面中使得方差最大，第三个轴也是在与第1,2个轴正交的平面中使得方差最大，这种假设在N维空间中，我们就可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间，但是我们可以选择r个坐标轴使得空间的压缩使得数据的损失最小。 假设我们矩阵的每一行代表一个样本，每一列代表一个feature，将一个m*n的矩阵A进行坐标轴的变化，P就是一个变换的矩阵从一个n维的空间变换到另外一个n维的空间 而将一个m*n的矩阵A变成一个m*r的矩阵，我们就会使得本来有n个feature的，变成有r个feature了(r小于n)，这r个其实就是对n个feature的一种提炼，我们把这个称为feature的压缩： 之前的SVD的式子是： 在矩阵的两边同时乘上一个矩阵V，由于v是一个正交的矩阵 我们对SVD分解的式子两边乘以U的转置矩阵U&amp;rsquo; PCA几乎可以说是对SVD的一种包装，如果我们实现了SVD，那也就实现了PCA。" />
    <meta name="twitter:image" content="https://cdn.jsdelivr.net/img/avatar.jpg" />
  

  
  <meta property="og:type" content="article" />
  <meta property="og:title" content="奇异值分解基础(SVD)" />
  <meta property="og:description" content="最近要了解一下Incremental PCA的一些知识，然后看到一篇论文里面讲到了SVD（奇异值分解），奈何自己以前没有把机器学习的课好好上，现在很多东西还是要补回来。所以，我就想了解一些SVD的基础知识。 PCA的实现一般有两种方法，一种是用特征值分解去实现，一种是用奇异值分解去实现的，SVD貌似在很多领域都有很重要的应用。
特征值和特征向量 特征值和特征向量是线性代数里面的基础知识，相信大部分人都知道： 很显然，λ就是特征向量v对应的特征值，一个矩阵的一组特征向量都是相互正交的，相信这些大家在线性代数都有学习。特征值分解是将一个矩阵以下面的形式进行分解： 其中Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角矩阵，每一个对角线上的元素就是一个特征值。 特征值分解可以得到特征值和特征向量，特征值表示的是这个特征值的重要性，而特征向量表示的是这个特征是什么，可以将每一个特征向量理解为一个线性的子空间。不过特征值分解也有很多的局限，比如变换的矩阵必须是方阵。
奇异值 特征值分解只能针对于方阵，局限性较大，而奇异值分解是一个能够用于任意的矩阵的一种分解方法： 假设A是一个N*M的矩阵，那么U是一个N*N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N*M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V&amp;rsquo;（V的转置矩阵）是一个N*N的矩阵，里面的向量也是正教的，称为右奇异向量。 我们将矩阵A和他的转置矩阵相乘，就可以得到一个方阵，我们利用方阵的求特征值可以得到： 这里面的v，就是我们上面所说的右奇异向量，由此我们可以得到 这里的σ就是上面所说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从打到小排列，而且σ的减少特别的快。在很多情况下，前10%的甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们可以用前r大的奇异值来近似描述矩阵，因此部分奇异值分解可以如下定义： r是一个远小于m、n的数， 右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，r越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和要远远小于原始的矩阵A。
SVD和PCA PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于机器学习的数据，方差大反而有意义，不然输入的数据就是同一个点了，那方差九尾0了。 这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假设我们想用一条直线去拟合这些点，那我们应该选择什么方向的线？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴和y轴上得到的方差就是相似的。 一般来说方差大的方向就是信号的方向，方差小的方向就是噪声的方向，我们在数据挖掘或者数字信号处理中，往往是要提高信噪比。就上图说，如果我们只保留signal方向的数据，就可以对原始数据进行不错的近似了。 PCA的就是对原始的空间中顺序地找一组相互正教的坐标轴，第一个轴使得方差最大，第二个轴是在与第一个轴相交的平面中使得方差最大，第三个轴也是在与第1,2个轴正交的平面中使得方差最大，这种假设在N维空间中，我们就可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间，但是我们可以选择r个坐标轴使得空间的压缩使得数据的损失最小。 假设我们矩阵的每一行代表一个样本，每一列代表一个feature，将一个m*n的矩阵A进行坐标轴的变化，P就是一个变换的矩阵从一个n维的空间变换到另外一个n维的空间 而将一个m*n的矩阵A变成一个m*r的矩阵，我们就会使得本来有n个feature的，变成有r个feature了(r小于n)，这r个其实就是对n个feature的一种提炼，我们把这个称为feature的压缩： 之前的SVD的式子是： 在矩阵的两边同时乘上一个矩阵V，由于v是一个正交的矩阵 我们对SVD分解的式子两边乘以U的转置矩阵U&amp;rsquo; PCA几乎可以说是对SVD的一种包装，如果我们实现了SVD，那也就实现了PCA。" />
  <meta property="og:url" content="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/post/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E5%9F%BA%E7%A1%80svd/" />
  <meta property="og:image" content="https://cdn.jsdelivr.net/img/avatar.jpg" />




<meta name="generator" content="Hugo 0.46" />


<link rel="canonical" href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/post/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E5%9F%BA%E7%A1%80svd/" />
<link rel="alternative" href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/index.xml" title="Neal&#39;s Blog" type="application/atom+xml" />


<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<meta name="format-detection" content="telephone=no,email=no,adress=no" />
<meta http-equiv="Cache-Control" content="no-transform" />


<meta content="" name="keywords">
<meta name="robots" content="index,follow" />
<meta name="referrer" content="origin-when-cross-origin" />
<meta name="google-site-verification" content="_moDmnnBNVLBN1rzNxyGUGdPHE20YgbmrtzLIbxaWFc" />
<meta name="msvalidate.01" content="22596E34341DD1D17D6022C44647E587" />





<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="apple-mobile-web-app-title" content="Neal&#39;s Blog" />
<meta name="msapplication-tooltip" content="Neal&#39;s Blog" />
<meta name='msapplication-navbutton-color' content="#5fbf5e" />
<meta name="msapplication-TileColor" content="#5fbf5e" />
<meta name="msapplication-TileImage" content="/img/tile-image-windows.png" />
<link rel="icon" href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/img/favicon.ico" />
<link rel="icon" type="image/png" sizes="16x16" href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/img/favicon-16x16.png" />
<link rel="icon" type="image/png" sizes="32x32" href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/img/favicon-32x32.png" />
<link rel="icon" sizes="192x192" href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/img/touch-icon-android.png" />
<link rel="apple-touch-icon" href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/img/touch-icon-apple.png" />
<link rel="mask-icon" href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/img/safari-pinned-tab.svg" color="#5fbf5e" />



<link rel="stylesheet" href="//cdn.bootcss.com/video.js/6.2.8/alt/video-js-cdn.min.css" />

<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/css/bundle.css" />


  
  <!--[if lt IE 9]>
    <script src="//cdn.bootcss.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="//cdn.bootcss.com/respond.js/1.4.2/respond.min.js"></script>
    <script src="//cdn.bootcss.com/video.js/6.2.8/ie8/videojs-ie8.min.js"></script>
  <![endif]-->

<!--[if lte IE 11]>
    <script src="//cdn.bootcss.com/classlist/1.1.20170427/classList.min.js"></script>
  <![endif]-->


<script src="//cdn.bootcss.com/object-fit-images/3.2.3/ofi.min.js"></script>


<script src="//cdn.bootcss.com/smooth-scroll/12.1.4/js/smooth-scroll.polyfills.min.js"></script>


</head>
  <body>
    
    <div class="suspension">
      <a title="Go to top" class="to-top is-hide"><span class="icon icon-up"></span></a>
      
        
        <a title="Go to comments" class="to-comment" href="#disqus_thread"><span class="icon icon-comment"></span></a>
        
      
    </div>
    
    
  <header class="site-header">
  <img class="avatar" src="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/img/avatar.jpg" alt="Avatar">
  
  <h2 class="title">Neal&#39;s Blog</h2>
  
  <p class="subtitle">Development &amp; Security</p>
  <button class="menu-toggle" type="button">
    <span class="icon icon-menu"></span>
  </button>
  <nav class="site-menu collapsed">
    <h2 class="offscreen">Main Menu</h2>
    <ul class="menu-list">
      
      
      
      
        <li class="menu-item
            
            
            
              is-active
            ">
            <a href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/">Home</a>
          </li>
      
        <li class="menu-item
            
            
            ">
            <a href="https://github.com/neal1991">Works</a>
          </li>
      
        <li class="menu-item
            
            
            ">
            <a href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/tags/">Tags</a>
          </li>
      
    </ul>
  </nav>
  <img src="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/img/wechat.jpg" alt="wechat" style="width: 240px;height: 240px;">
  <nav class="social-menu collapsed">
    <h2 class="offscreen">Social Networks</h2>
    <ul class="social-list">

      
      <li class="social-item">
        <a href="mailto:bing.ecnu@gmail.com" title="Email"><span class="icon icon-email"></span></a>
      </li>

      
      <li class="social-item">
        <a href="//github.com/neal1991" title="GitHub"><span class="icon icon-github"></span></a>
      </li>

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <li class="social-item">
        <a href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/img/wechat.jpg" title="Wechat"><span class="icon icon-wechat"></span></a>
      </li>

      

      

      

      

      <li class="social-item">
        <a href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/index.xml"><span class="icon icon-rss" title="RSS"></span></a>
      </li>

    </ul>
  </nav>
</header>

  <section class="main post-detail">
    <header class="post-header">
      <h1 class="post-title">奇异值分解基础(SVD)</h1>
      <p class="post-meta">@Neal · Jun 27, 2015 · 1 min read</p>
    </header>
    <article class="post-content">

<p>最近要了解一下Incremental PCA的一些知识，然后看到一篇论文里面讲到了SVD（奇异值分解），奈何自己以前没有把机器学习的课好好上，现在很多东西还是要补回来。所以，我就想了解一些SVD的基础知识。
PCA的实现一般有两种方法，一种是用特征值分解去实现，一种是用奇异值分解去实现的，SVD貌似在很多领域都有很重要的应用。</p>

<h2 id="特征值和特征向量">特征值和特征向量</h2>

<p>特征值和特征向量是线性代数里面的基础知识，相信大部分人都知道：
<img src="http://img.blog.csdn.net/20150627154504441" alt="这里写图片描述" />
很显然，λ就是特征向量v对应的特征值，一个矩阵的一组特征向量都是相互正交的，相信这些大家在线性代数都有学习。特征值分解是将一个矩阵以下面的形式进行分解：
<img src="http://img.blog.csdn.net/20150627154713620" alt="这里写图片描述" />
其中Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角矩阵，每一个对角线上的元素就是一个特征值。
特征值分解可以得到特征值和特征向量，特征值表示的是这个特征值的重要性，而特征向量表示的是这个特征是什么，可以将每一个特征向量理解为一个线性的子空间。不过特征值分解也有很多的局限，比如变换的矩阵必须是方阵。</p>

<h2 id="奇异值">奇异值</h2>

<p>特征值分解只能针对于方阵，局限性较大，而奇异值分解是一个能够用于任意的矩阵的一种分解方法：
<img src="http://img.blog.csdn.net/20150627155554117" alt="这里写图片描述" />
假设A是一个N*M的矩阵，那么U是一个N*N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N*M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V&rsquo;（V的转置矩阵）是一个N*N的矩阵，里面的向量也是正教的，称为右奇异向量。
<img src="http://img.blog.csdn.net/20150627160123326" alt="这里写图片描述" />
我们将矩阵A和他的转置矩阵相乘，就可以得到一个方阵，我们利用方阵的求特征值可以得到：
<img src="http://img.blog.csdn.net/20150627160318055" alt="这里写图片描述" />
这里面的v，就是我们上面所说的右奇异向量，由此我们可以得到
<img src="http://img.blog.csdn.net/20150627160432947" alt="这里写图片描述" />
这里的σ就是上面所说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从打到小排列，而且σ的减少特别的快。在很多情况下，前10%的甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们可以用前r大的奇异值来近似描述矩阵，因此部分奇异值分解可以如下定义：
<img src="http://img.blog.csdn.net/20150627161252436" alt="这里写图片描述" />
r是一个远小于m、n的数，
<img src="http://img.blog.csdn.net/20150627161545868" alt="这里写图片描述" />
右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，r越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和要远远小于原始的矩阵A。</p>

<h2 id="svd和pca">SVD和PCA</h2>

<p>PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于机器学习的数据，方差大反而有意义，不然输入的数据就是同一个点了，那方差九尾0了。
<img src="http://img.blog.csdn.net/20150627162402286" alt="这里写图片描述" />
这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假设我们想用一条直线去拟合这些点，那我们应该选择什么方向的线？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴和y轴上得到的方差就是相似的。
一般来说方差大的方向就是信号的方向，方差小的方向就是噪声的方向，我们在数据挖掘或者数字信号处理中，往往是要提高信噪比。就上图说，如果我们只保留signal方向的数据，就可以对原始数据进行不错的近似了。
PCA的就是对原始的空间中顺序地找一组相互正教的坐标轴，第一个轴使得方差最大，第二个轴是在与第一个轴相交的平面中使得方差最大，第三个轴也是在与第1,2个轴正交的平面中使得方差最大，这种假设在N维空间中，我们就可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间，但是我们可以选择r个坐标轴使得空间的压缩使得数据的损失最小。
假设我们矩阵的每一行代表一个样本，每一列代表一个feature，将一个m*n的矩阵A进行坐标轴的变化，P就是一个变换的矩阵从一个n维的空间变换到另外一个n维的空间
<img src="http://img.blog.csdn.net/20150627163615343" alt="这里写图片描述" />
<img src="http://img.blog.csdn.net/20150627163834504" alt="这里写图片描述" />
而将一个m*n的矩阵A变成一个m*r的矩阵，我们就会使得本来有n个feature的，变成有r个feature了(r小于n)，这r个其实就是对n个feature的一种提炼，我们把这个称为feature的压缩：
之前的SVD的式子是：
<img src="http://img.blog.csdn.net/20150627161252436" alt="这里写图片描述" />
在矩阵的两边同时乘上一个矩阵V，由于v是一个正交的矩阵
<img src="http://img.blog.csdn.net/20150627165333436" alt="这里写图片描述" />
我们对SVD分解的式子两边乘以U的转置矩阵U&rsquo;
<img src="http://img.blog.csdn.net/20150627180627671" alt="这里写图片描述" />
PCA几乎可以说是对SVD的一种包装，如果我们实现了SVD，那也就实现了PCA。</p>
</article>
    <footer class="post-footer">
        <p>欢迎搜索微信号 mad_coder 或者扫描二维码关注公众号：</p>
        <img src="https://s2.ax1x.com/2019/08/19/m1jSLd.jpg" alt="m1jSLd.jpg">
      
      <ul class="post-tags">
        
          <li><a href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/tags/pca"><span class="tag">Pca</span></a></li>
        
          <li><a href="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/tags/svd"><span class="tag">Svd</span></a></li>
        
      </ul>
      
      <p class="post-copyright">
        © This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License，please give source if you wish to quote or reproduce.This post was published <strong>1770</strong> days ago, content in the post may be inaccurate, even wrong now, please take risk yourself.
      </p>
    </footer>
    
      <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "nealecnu" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      
    
  </section>
  <footer class="site-footer">
  <p>© 2017-2020 Neal&#39;s Blog</p>
</footer>



<script async src="//cdn.bootcss.com/video.js/6.2.8/alt/video.novtt.min.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[','\\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
      extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
  });
</script>
<script type="text/x-mathjax-config">
  // Fix <code> tags after MathJax finishes running. This is a
  // hack to overcome a shortcoming of Markdown. Discussion at
  // https://github.com/mojombo/jekyll/issues/199
  MathJax.Hub.Queue(() => {
    MathJax.Hub.getAllJax().map(v => v.SourceElement().parentNode.className += ' has-jax');
  });
</script>

<script src="https://cdn.jsdelivr.net/gh/neal1991/neal1991.github.io/js/bundle.js"></script>


<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-104405154-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>





  </body>
</html>
